# LangChain RAG Chatbot with OpenAI + ChromaDB

Este proyecto es una implementaci√≥n pr√°ctica de un sistema **RAG** (Retrieval-Augmented Generation) utilizando **LangChain**, **ChromaDB** y **OpenAI**. Permite consultar documentos locales (en este caso, un `.md` del libro *Alice in Wonderland*) usando lenguaje natural, y obtener respuestas generadas por una IA con base en el contenido real.

---

## üß† ¬øQu√© es RAG?

RAG combina:

- üîç Recuperaci√≥n de informaci√≥n (vector search): busca los fragmentos m√°s relevantes en tus documentos.
- üß† Generaci√≥n con LLM (modelo de lenguaje): genera una respuesta en lenguaje natural bas√°ndose en esos fragmentos.

---

## üõ†Ô∏è Requisitos previos

- Python 3.10+  
- Cuenta en [OpenAI](https://platform.openai.com/) y una API key v√°lida.
- Editor de texto (VS Code recomendado).
- Conexi√≥n a Internet para las llamadas a OpenAI.

---

## üì¶ Instalaci√≥n paso a paso

1. **Clona este repositorio y entra en el proyecto**

```bash
git clone https://github.com/tu_usuario/tu_repo.git
cd rag-project
```

2. **Crea y activa un entorno virtual**

```bash
python -m venv venv
.env\Scriptsctivate   # En Windows
source venv/bin/activate # En macOS/Linux
```

3. **Instala las dependencias principales**

> ‚ö†Ô∏è Si usas Windows y obtienes errores con `onnxruntime`, revisa m√°s abajo los pasos espec√≠ficos.

```bash
pip install -r requirements.txt
```

4. **Instala soporte para Markdown (muy importante para leer los `.md`)**

```bash
pip install "unstructured[md]"
```

5. **Crea un archivo `.env` y a√±ade tu clave de OpenAI:**

```
OPENAI_API_KEY=sk-xxxxxx
```

---

## ü™õ Posibles errores y soluciones

### ‚ùå `Permission denied` (en Windows)
‚û° Ejecuta `pip install` con `--no-cache-dir`:
```bash
pip install chromadb --no-cache-dir
```

### ‚ùå `ModuleNotFoundError: No module named 'unstructured'`
‚û° Instala con:
```bash
pip install "unstructured[md]"
```

### ‚ùå `LangChainDeprecationWarning: predict is deprecated`
‚û° Usa `.invoke()` en lugar de `.predict()` en tus scripts.

### ‚ùå `libmagic is unavailable`
‚û° Es solo una advertencia. Puedes ignorarla, pero si quieres soporte completo en detecci√≥n de archivos, instala `libmagic` manualmente (opcional).

---

## üß™ Uso del sistema

### 1. Crear la base de datos vectorial (embedding + almacenamiento)

```bash
python create_database.py
```

Esto divide el documento en chunks y los guarda como vectores en `chroma/`.

### 2. Consultar a la IA con tu propia pregunta

```bash
python query_data.py "How does Alice meet the Mad Hatter?"
```

---

## üåê Integraci√≥n con Frontend en React

Si ya tienes un frontend con React (por ejemplo, un chat que use OpenAI), puedes conectar este backend RAG usando **FastAPI**.

### Backend con FastAPI

Instala FastAPI y Uvicorn:

```bash
pip install fastapi uvicorn
```

Crea un archivo `backend.py`:

```python
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate

app = FastAPI()

class Question(BaseModel):
    query: str

CHROMA_PATH = "chroma"
PROMPT_TEMPLATE = '''
Answer the question using only the following context:

{context}

---

Question: {question}
'''

@app.post("/ask")
async def ask_question(data: Question):
    embedding_function = OpenAIEmbeddings()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)
    results = db.similarity_search_with_relevance_scores(data.query, k=3)

    if not results or results[0][1] < 0.7:
        return {"answer": "No relevant information found."}

    context = "\n\n---\n\n".join([doc.page_content for doc, _ in results])
    prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE).format(context=context, question=data.query)
    model = ChatOpenAI()
    response = model.invoke(prompt).content
    return {"answer": response}
```

Lanza el backend:

```bash
uvicorn backend:app --reload
```

### En tu frontend React:

Haz una llamada al backend:

```js
const response = await fetch("http://localhost:8000/ask", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({ query: "Your user question" })
});

const data = await response.json();
console.log(data.answer);
```

---

## üìÅ Estructura esperada del proyecto

```
rag-project/
‚îú‚îÄ‚îÄ chroma/                    ‚Üê base de datos vectorial
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ books/
‚îÇ       ‚îî‚îÄ‚îÄ alice_in_wonderland.md
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ create_database.py
‚îú‚îÄ‚îÄ query_data.py
‚îú‚îÄ‚îÄ compare_embeddings.py
‚îú‚îÄ‚îÄ backend.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üìπ Tutorial original en video

Basado en este excelente tutorial:  
üé• [RAG + Langchain Python Project - YouTube](https://www.youtube.com/watch?v=tcqEUSNCn8I)

---

## ‚úÖ Cr√©ditos y mejoras

Proyecto extendido por Oscar con mejoras de compatibilidad, correcci√≥n de deprecaciones, conexi√≥n con frontend y documentaci√≥n clara.  
¬°Listo para usar en tus propios documentos, manuales, libros o FAQs!
